{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "_BSx5j2GxeDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp9NRCuksd6L"
      },
      "outputs": [],
      "source": [
        "class Main():\n",
        "  def __init__(self):\n",
        "        # Constructor method\n",
        "        from google.colab import drive\n",
        "\n",
        "        #Mounting to Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "  # def readingDataFromCSV(self):\n",
        "  #       import csv\n",
        "  #       import numpy as np\n",
        "  #       #Reading the data from CSV\n",
        "  #       inputs=[]\n",
        "  #       with open('/content/drive/MyDrive/cleanDataWithNameChnagedToNumber.csv', newline='') as csvfile:\n",
        "  #           reader = csv.reader(csvfile, delimiter=',')\n",
        "  #           for row in reader:\n",
        "  #             inputs.append(row)\n",
        "\n",
        "  #       inputs= np.array(inputs).astype(float)\n",
        "\n",
        "\n",
        "  #       #Reading Label CSV\n",
        "  #       label_data=[]\n",
        "  #       with open('/content/drive/MyDrive/Label.csv', newline='') as csvfile:\n",
        "  #         reader = csv.reader(csvfile, delimiter=',')\n",
        "  #         for row in reader:\n",
        "  #           label_data.append(row)\n",
        "\n",
        "  #       label_data = np.array(label_data).astype(float)\n",
        "\n",
        "  #       return inputs,label_data\n",
        "\n",
        "  # def dataSplit(self,inputs,label_data):\n",
        "  def dataSplit(self):\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          from sklearn.utils import shuffle\n",
        "          import numpy as np\n",
        "          \n",
        "          # # Load the .npy file\n",
        "          # my_array = np.load('/content/drive/MyDrive/train_data.npy')\n",
        "\n",
        "          # # Print the shape of the array\n",
        "          # print(my_array.shape)\n",
        "\n",
        "          # Load the .npy file\n",
        "\n",
        "          my_array = np.load('/content/drive/MyDrive/train_data.npy')\n",
        "          my_array1 = np.load('/content/drive/MyDrive/y_train.npy')\n",
        "          my_array2 = np.load('/content/drive/MyDrive/test_data.npy')\n",
        "          my_array3 = np.load('/content/drive/MyDrive/y_test.npy')\n",
        "          train_data = np.append(my_array, my_array2, axis=0)\n",
        "          label_data = np.append(my_array1, my_array3, axis=0)\n",
        "          train_data_copy, test_data, train_label_data_copy,test_label_data = train_test_split(train_data,label_data, train_size=40000, test_size=5714)\n",
        "\n",
        "          print(my_array1.shape)\n",
        "          print(train_label_data_copy)\n",
        "                    \n",
        "\n",
        "          # train_data_copy, test_data, train_label_data_copy,test_label_data = train_test_split(inputs,label_data, train_size=40000, test_size=5000,random_state=42)\n",
        "          \n",
        "          # train_data_copy, test_data= train_test_split(inputs, train_size=40000, test_size=5000)\n",
        "\n",
        "          # train_label_data_copy, test_label_data=train_test_split(label_data, train_size=40000,test_size=5000)\n",
        "\n",
        "          #Shuffle\n",
        "          # train_data_copy, train_label_data_copy = shuffle(train_data_copy, train_label_data_copy, random_state=42)\n",
        "\n",
        "          #Shuffle\n",
        "          # test_data, test_label_data = shuffle(test_data, test_label_data, random_state=42)\n",
        "\n",
        "          return train_data_copy,test_data,train_label_data_copy,test_label_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnnPS4PPz8qR"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "  def modelDef(units):\n",
        "    import tensorflow as tf\n",
        "\n",
        "    #Define the architecture of the model\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units, activation='relu', input_shape=(24,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    #Compile the mode\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "  # Compile the model\n",
        "  #def modelCompile():    \n",
        "    \n",
        "  #Manually Set the weights\n",
        "  def ModelSetWeights(model,randomWeightsForLayer):\n",
        "   model.layers[0].set_weights(randomWeightsForLayer[0])\n",
        "   model.layers[1].set_weights(randomWeightsForLayer[1])\n",
        "\n",
        "   return model\n",
        "\n",
        "  def ModelFit(model,temp_train,temp_label):\n",
        "    model.fit(temp_train, temp_label, epochs=4)\n",
        "    weightsZero = model.layers[0].get_weights()\n",
        "    weightsOne=model.layers[1].get_weights()\n",
        "    return weightsZero,weightsOne,model\n",
        "\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vzPgWaZgefU"
      },
      "outputs": [],
      "source": [
        "class Server(Model):\n",
        "  def randomWeights(self,units):\n",
        "    model=Model.modelDef(units)\n",
        "    randomWeightsForLayer=[]\n",
        "    randomWeightsForLayerZero=[]\n",
        "    randomWeightsForLayerOne=[]\n",
        "    randomWeightsForLayerZero.append(model.layers[0].get_weights()[0])\n",
        "    randomWeightsForLayerZero.append(model.layers[0].get_weights()[1])\n",
        "    randomWeightsForLayerOne.append(model.layers[1].get_weights()[0])\n",
        "    randomWeightsForLayerOne.append(model.layers[1].get_weights()[1])\n",
        "    randomWeightsForLayer.append(randomWeightsForLayerZero)\n",
        "    randomWeightsForLayer.append(randomWeightsForLayerOne)\n",
        "\n",
        "    #print(model.layers[0].get_weights)\n",
        "    return randomWeightsForLayer\n",
        "\n",
        "  def dropAvg(self,weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne,dropClient):\n",
        "    averageWeightsLayerZero=np.zeros_like(weightAfterTraining[0])\n",
        "\n",
        "\n",
        "    for i in range(len(weightAfterTraining)):\n",
        "      if(dropClient==i):\n",
        "        continue\n",
        "      averageWeightsLayerZero=np.add(averageWeightsLayerZero,weightAfterTraining[i])\n",
        "\n",
        "    averageWeightsLayerZero=averageWeightsLayerZero/numberOfClients-1\n",
        "    #print(averageWeightsLayerZero)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    averageWeightsLayerOne=np.zeros_like(weightAfterTrainingOne[0])\n",
        "\n",
        "    for i in range(len(weightAfterTrainingOne)):\n",
        "      if(dropClient==i):\n",
        "        continue\n",
        "      averageWeightsLayerOne=np.add(averageWeightsLayerOne,weightAfterTrainingOne[i])\n",
        "    \n",
        "    averageWeightsLayerOne=averageWeightsLayerOne/numberOfClients-1\n",
        "    #print(averageWeightsLayerOne)\n",
        "\n",
        "\n",
        "    \n",
        "    averageBiasLayerZero=np.zeros_like(biasAfterTraining[0])\n",
        "\n",
        "    for i in range(len(biasAfterTraining)):\n",
        "      if(dropClient==i):\n",
        "        continue\n",
        "      averageBiasLayerZero=np.add(averageBiasLayerZero,biasAfterTraining[i])\n",
        "\n",
        "    averageBiasLayerZero=averageBiasLayerZero/numberOfClients-1\n",
        "    #print(averageBiasLayerZero)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    averageBiasLayerOne = np.zeros_like(biasAfterTrainingOne[0])\n",
        "\n",
        "    for i in range(len(biasAfterTrainingOne)):\n",
        "      if(dropClient==i):\n",
        "        continue\n",
        "      averageBiasLayerOne=np.add(averageBiasLayerOne,biasAfterTrainingOne[i])\n",
        "\n",
        "    averageBiasLayerOne=averageBiasLayerOne/numberOfClients-1\n",
        "    #print(averageBiasLayerOne)\n",
        "\n",
        "    \n",
        "    \n",
        "    #print(weightAfterTraining)\n",
        "    #print(biasAfterTraining)\n",
        "    WeightsForServerLayerZero=[]\n",
        "    WeightsForServerLayerOne=[]\n",
        "    WeightsForServerLayerZero.append(averageWeightsLayerZero)\n",
        "    WeightsForServerLayerZero.append(averageBiasLayerZero)\n",
        "    WeightsForServerLayerOne.append(averageWeightsLayerOne)\n",
        "    WeightsForServerLayerOne.append(averageBiasLayerOne)\n",
        "    WeightsForServer=[]\n",
        "    WeightsForServer.append(WeightsForServerLayerZero)\n",
        "    WeightsForServer.append(WeightsForServerLayerOne)\n",
        "\n",
        "    #WeightsForServer=server.calculateAvg(weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne)\n",
        "    return WeightsForServer\n",
        "\n",
        "\n",
        "\n",
        "  def calculateAvg(self,weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne):\n",
        "    averageWeightsLayerZero=np.zeros_like(weightAfterTraining[0])\n",
        "\n",
        "\n",
        "    for i in range(len(weightAfterTraining)):\n",
        "      averageWeightsLayerZero=np.add(averageWeightsLayerZero,weightAfterTraining[i])\n",
        "\n",
        "    averageWeightsLayerZero=averageWeightsLayerZero/numberOfClients\n",
        "    #print(averageWeightsLayerZero)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    averageWeightsLayerOne=np.zeros_like(weightAfterTrainingOne[0])\n",
        "\n",
        "    for i in range(len(weightAfterTrainingOne)):\n",
        "      averageWeightsLayerOne=np.add(averageWeightsLayerOne,weightAfterTrainingOne[i])\n",
        "    \n",
        "    averageWeightsLayerOne=averageWeightsLayerOne/numberOfClients\n",
        "    #print(averageWeightsLayerOne)\n",
        "\n",
        "\n",
        "    \n",
        "    averageBiasLayerZero=np.zeros_like(biasAfterTraining[0])\n",
        "\n",
        "    for i in range(len(biasAfterTraining)):\n",
        "      averageBiasLayerZero=np.add(averageBiasLayerZero,biasAfterTraining[i])\n",
        "\n",
        "    averageBiasLayerZero=averageBiasLayerZero/numberOfClients\n",
        "    #print(averageBiasLayerZero)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    averageBiasLayerOne = np.zeros_like(biasAfterTrainingOne[0])\n",
        "\n",
        "    for i in range(len(biasAfterTrainingOne)):\n",
        "      averageBiasLayerOne=np.add(averageBiasLayerOne,biasAfterTrainingOne[i])\n",
        "\n",
        "    averageBiasLayerOne=averageBiasLayerOne/numberOfClients\n",
        "    #print(averageBiasLayerOne)\n",
        "\n",
        "    \n",
        "    \n",
        "    #print(weightAfterTraining)\n",
        "    #print(biasAfterTraining)\n",
        "    WeightsForServerLayerZero=[]\n",
        "    WeightsForServerLayerOne=[]\n",
        "    WeightsForServerLayerZero.append(averageWeightsLayerZero)\n",
        "    WeightsForServerLayerZero.append(averageBiasLayerZero)\n",
        "    WeightsForServerLayerOne.append(averageWeightsLayerOne)\n",
        "    WeightsForServerLayerOne.append(averageBiasLayerOne)\n",
        "    WeightsForServer=[]\n",
        "    WeightsForServer.append(WeightsForServerLayerZero)\n",
        "    WeightsForServer.append(WeightsForServerLayerOne)\n",
        "\n",
        "    #WeightsForServer=server.calculateAvg(weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    return WeightsForServer\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcSuPddq1U_Q"
      },
      "outputs": [],
      "source": [
        "class Client(Model,Main):\n",
        "  def trainClient(self,numberOfClients,randomWeights,units,per,train_data_copy,train_label_data_copy,dropClient):\n",
        "      train_data,train_label_data=Client.dataPartition(numberOfClients,train_data_copy,train_label_data_copy,per,dropClient)\n",
        "      weightAfterTrainingForLayerOne = []\n",
        "      biasAfterTrainingForLayerOne = []\n",
        "      weightAfterTrainingOne = []\n",
        "      biasAfterTrainingOne = []\n",
        "      \n",
        "      if(per==2):\n",
        "        numberOfClients-=1\n",
        "      for i in range(numberOfClients):\n",
        "        model=Model.modelDef(units)\n",
        "        model=Model.ModelSetWeights(model,randomWeights)\n",
        "        weightsAfterTraining,weightsOne,model=Model.ModelFit(model,train_data[i],train_label_data[i])\n",
        "        weightAfterTrainingForLayerOne.append(weightsAfterTraining[0])\n",
        "        biasAfterTrainingForLayerOne.append(weightsAfterTraining[1])\n",
        "        weightAfterTrainingOne.append(weightsOne[0])\n",
        "        biasAfterTrainingOne.append(weightsOne[1])\n",
        "      return weightAfterTrainingForLayerOne,biasAfterTrainingForLayerOne,weightAfterTrainingOne,biasAfterTrainingOne\n",
        "\n",
        "\n",
        "  def dataPartition(numberOfClients,train_data_copy,train_label_data_copy,per,dropClient):\n",
        "    import random\n",
        "    data = 40000\n",
        "    data_per_user = data // numberOfClients\n",
        "  \n",
        " \n",
        "    if(per==1):\n",
        "      train_label_data_copy=Client.dataManipulation(numberOfClients,train_label_data_copy)\n",
        "\n",
        "    #shuffled_label_data=dataManipulation(clientNumber,percentageOfData,shuffled_label_data)\n",
        "    train_data = []\n",
        "    train_label_data=[]\n",
        "    j=0\n",
        "    k=data_per_user\n",
        "    for i in range(numberOfClients):\n",
        "      if(per==2 and i==dropClient):\n",
        "        j=j+data_per_user\n",
        "        k=k+data_per_user\n",
        "        continue\n",
        "      train_data.append(train_data_copy[j:k])\n",
        "      train_label_data.append( train_label_data_copy[j:k])\n",
        "      j=j+data_per_user\n",
        "      k=k+data_per_user\n",
        "\n",
        "    return train_data,train_label_data\n",
        "\n",
        "\n",
        "\n",
        "  def dataManipulation(numberOfClients,shuffled_label_data):\n",
        "    clientNumber=int(input(\"Enter Client Number:\"))\n",
        "    percentage=int(input(\"Enter Percentage Of Data You want To Manipulate:\"))\n",
        "    dataPerUser=40000//numberOfClients\n",
        "    j=int(dataPerUser*(clientNumber-1))\n",
        "    k=int(j+(dataPerUser*percentage)/100)\n",
        "    #print(j)\n",
        "    #print(k)\n",
        "    print(\"Amount of Data Changed: \",k-j)\n",
        "    \n",
        "    for i in range(j,k):\n",
        "      if(shuffled_label_data[i]==0):\n",
        "        shuffled_label_data[i]=1;\n",
        "      else:\n",
        "        shuffled_label_data[i]=0;\n",
        "\n",
        "    return shuffled_label_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xEFzd041ZAb"
      },
      "outputs": [],
      "source": [
        "class Finding_Anomaly():\n",
        "\n",
        "  def kMeansClustering(self,clusterSize,normalizedWeights):\n",
        "    from sklearn.cluster import KMeans\n",
        "\n",
        "    # Load your dataset into X\n",
        "    X = normalizedWeights\n",
        "\n",
        "    # Define the number of clusters\n",
        "    #k = 2\n",
        "\n",
        "    # Create a KMeans object with k clusters\n",
        "    kmeans = KMeans(n_clusters=clusterSize)\n",
        "\n",
        "    # Fit the KMeans model on the data\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Get the cluster assignments for each data point\n",
        "    labels = kmeans.labels_\n",
        "    #print(labels)\n",
        "\n",
        "    # Get the cluster centers\n",
        "    centers = kmeans.cluster_centers_\n",
        "\n",
        "    return labels,centers\n",
        "\n",
        "\n",
        "\n",
        "  def euclideanDistance(self,weights_array):\n",
        "    weights_mean = np.mean(weights_array, axis=0)\n",
        "    distances=[]\n",
        "    for i in range(len(weightAfterTraining)):\n",
        "      distance_1_2 = np.linalg.norm(weights_mean - weightAfterTraining[i])\n",
        "      distances.append(distance_1_2)\n",
        "      print(distance_1_2)\n",
        "\n",
        "    return distances\n",
        "\n",
        "  def cosineSimilarity(self,a,b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "\n",
        "  def manhattanDistance(self,weights_array):\n",
        "    weights_mean = np.mean(weights_array, axis=0)\n",
        "    distances = []\n",
        "    for weight in weights_array:\n",
        "        distance = np.sum(np.abs(weight - weights_mean))\n",
        "        distances.append(distance)\n",
        "        print(distance)\n",
        "    return distances\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LA3-f2QtbG9",
        "outputId": "45978fde-0491-4bd3-d275-71e90233df01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(40000,)\n",
            "[0 1 1 ... 1 0 1]\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6156\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5553\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5427\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5375\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6327\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5816\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5708\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5655\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6419\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5885\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5761\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5712\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 3ms/step - loss: 0.6419\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5876\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5733\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5666\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6389\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5835\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5707\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5651\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6361\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5791\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5669\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5616\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6388\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5959\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5855\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5800\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6343\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5749\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5605\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5548\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6249\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5717\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5592\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5532\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6289\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5674\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5530\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5472\n",
            "(10, 24, 4)\n",
            "179/179 [==============================] - 0s 2ms/step\n",
            "Accuracy of the model Before Changing Data: 76.13%\n",
            "Enter Client Number:7\n",
            "Enter Percentage Of Data You want To Manipulate:10\n",
            "Amount of Data Changed:  400\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6174\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5542\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5422\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5371\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6337\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5816\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5706\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5652\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6409\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5894\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5766\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5713\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 3ms/step - loss: 0.6431\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5884\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5737\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5669\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6391\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5840\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5707\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5652\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6358\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5797\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5672\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5617\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6676\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.6376\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.6286\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.6243\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6280\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5720\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5596\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5542\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6250\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5719\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5593\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5534\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6293\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5676\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5531\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5470\n",
            "(10, 24, 4)\n",
            "Result Of Clusters:\n",
            "[0 0 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Anomoly found using Clustering: 7\n",
            "\n",
            "0.24908593\n",
            "0.32629988\n",
            "0.24326454\n",
            "0.30576876\n",
            "0.2647277\n",
            "0.2749668\n",
            "0.36565307\n",
            "0.33260196\n",
            "0.29519212\n",
            "0.24138461\n",
            "Result Of Euclidean:\n",
            "[0.24908593, 0.32629988, 0.24326454, 0.30576876, 0.2647277, 0.2749668, 0.36565307, 0.33260196, 0.29519212, 0.24138461]\n",
            "Anomoly found Using Euclidean: 7\n",
            "\n",
            "\n",
            "1.8836672\n",
            "2.348018\n",
            "1.8412602\n",
            "2.3662233\n",
            "2.0597901\n",
            "2.1237326\n",
            "2.8656516\n",
            "2.344946\n",
            "2.2448926\n",
            "1.8336172\n",
            "Result Of Manhattan:\n",
            "[1.8836672, 2.348018, 1.8412602, 2.3662233, 2.0597901, 2.1237326, 2.8656516, 2.344946, 2.2448926, 1.8336172]\n",
            "Anomoly found Using Manhatten: 7\n",
            "\n",
            "Anomoly Detected Using Weighted Average :  7\n",
            "\n",
            "USER:0 USER:1\n",
            "0.988958\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:2\n",
            "0.9907428\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:3\n",
            "0.99058247\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:4\n",
            "0.9909882\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:5\n",
            "0.98973274\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:6\n",
            "0.9843094\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:7\n",
            "0.9864956\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:8\n",
            "0.99053156\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:0 USER:9\n",
            "0.9944825\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:2\n",
            "0.9894191\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:3\n",
            "0.9867543\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:4\n",
            "0.98699665\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:5\n",
            "0.9863353\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:6\n",
            "0.9822771\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:7\n",
            "0.98197925\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:8\n",
            "0.98581374\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:1 USER:9\n",
            "0.98909414\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:3\n",
            "0.98817796\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:4\n",
            "0.9922924\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:5\n",
            "0.9896603\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:6\n",
            "0.986502\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:7\n",
            "0.9872423\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:8\n",
            "0.9913599\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:2 USER:9\n",
            "0.9925626\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:4\n",
            "0.9889159\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:5\n",
            "0.9882688\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:6\n",
            "0.98412293\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:7\n",
            "0.9873107\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:8\n",
            "0.9841367\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:3 USER:9\n",
            "0.9878461\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:4 USER:5\n",
            "0.9901422\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:4 USER:6\n",
            "0.9838619\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:4 USER:7\n",
            "0.9893842\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:4 USER:8\n",
            "0.98902935\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:4 USER:9\n",
            "0.9896509\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:5 USER:6\n",
            "0.986385\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:5 USER:7\n",
            "0.9857746\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:5 USER:8\n",
            "0.98899806\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:5 USER:9\n",
            "0.9933213\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:6 USER:7\n",
            "0.9827489\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:6 USER:8\n",
            "0.9834254\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:6 USER:9\n",
            "0.9866028\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:7 USER:8\n",
            "0.98760915\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:7 USER:9\n",
            "0.9864594\n",
            "!!!!!!!!!!!!!!!\n",
            "USER:8 USER:9\n",
            "0.99154854\n",
            "!!!!!!!!!!!!!!!\n",
            "(2, 2)\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2007: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179/179 [==============================] - 0s 2ms/step\n",
            "Accuracy of the model After Changing Data: 76.11%\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6152\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5540\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5420\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5375\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6336\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5831\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5715\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5659\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 2s 2ms/step - loss: 0.6390\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5886\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5765\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5713\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6460\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5883\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5734\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5667\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6394\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5836\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5711\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5657\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 3ms/step - loss: 0.6369\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5804\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5679\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 3ms/step - loss: 0.5621\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6302\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5732\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5600\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5543\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6266\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5725\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5595\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5535\n",
            "Epoch 1/4\n",
            "125/125 [==============================] - 1s 2ms/step - loss: 0.6308\n",
            "Epoch 2/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5683\n",
            "Epoch 3/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5531\n",
            "Epoch 4/4\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.5473\n",
            "179/179 [==============================] - 0s 1ms/step\n",
            "Accuracy of the model After dropping client: 23.75%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Create an instance of the Main class and run the program\n",
        "\n",
        "    #object of Main Class\n",
        "    main = Main()\n",
        "    import numpy as np\n",
        "\n",
        "    #Object Of Server Class\n",
        "    server=Server()\n",
        "\n",
        "    #Define Units\n",
        "    units=4\n",
        "    randomWeights=[]\n",
        "\n",
        "    #Generating Random Weights\n",
        "    randomWeights=server.randomWeights(units)\n",
        "\n",
        "    #print(randomWeights)\n",
        "\n",
        "\n",
        "    numberOfClients=10\n",
        "     #int(input(\"Enter The Number Of Clients:\"))\n",
        "    # numberOfClients=\n",
        "    client=Client()\n",
        "\n",
        "    # inputs,label_data=main.readingDataFromCSV()\n",
        "    train_data_copy,test_data,train_label_data_copy,test_label=main.dataSplit()\n",
        "    \n",
        "    #Weights After Training\n",
        "    weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne=client.trainClient(numberOfClients,randomWeights,units,0,train_data_copy,train_label_data_copy,0)\n",
        "    print(np.shape(weightAfterTraining))\n",
        "    WeightsForServer=server.calculateAvg(weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne)\n",
        "\n",
        "    model=Model()\n",
        "    model=Model.modelDef(4)\n",
        "    model=Model.ModelSetWeights(model,WeightsForServer)\n",
        "    # inputs,label_data=Main.readingDataFromCSV()\n",
        "    # train_data_copy,test_data,train_label_data_copy,test_label=Main.dataSplit(inputs,label_data)\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Obtain the predicted outputs of the model\n",
        "    y_pred = model.predict(test_data)\n",
        "\n",
        "    #print(y_pred)\n",
        "    #print(test_label)\n",
        "\n",
        "    for i in range(len(y_pred)):\n",
        "      if(y_pred[i]>0.5):\n",
        "        y_pred[i]=1\n",
        "      else:\n",
        "        y_pred[i]=0\n",
        "\n",
        "    #print(y_pred)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(test_label, y_pred)\n",
        "\n",
        "    print(\"Accuracy of the model Before Changing Data: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Forging the data\n",
        "\n",
        "\n",
        "    weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne=client.trainClient(numberOfClients,randomWeights,units,1,train_data_copy,train_label_data_copy,0)\n",
        "    print(np.shape(weightAfterTraining))\n",
        "    WeightsForServer1=server.calculateAvg(weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne)\n",
        "\n",
        "    import numpy as np\n",
        "    weights_array1 = np.array(weightAfterTraining)\n",
        "\n",
        "    flattendWeightedArray=[]\n",
        "    for i in range(numberOfClients):\n",
        "      flattendWeightedArray.append(weights_array1[i].reshape(units*24,))\n",
        "\n",
        "    mean = np.mean(flattendWeightedArray)\n",
        "    std = np.std(flattendWeightedArray)\n",
        "\n",
        "    # Normalize the weights using z-score normalization\n",
        "    normalizedWeights = (flattendWeightedArray - mean) / std\n",
        "\n",
        "    #Object Of Finding_Anomaly Class\n",
        "    anamolyFinder=Finding_Anomaly()\n",
        "\n",
        "    #Clustering Result\n",
        "    k=2\n",
        "    clusters,centers=anamolyFinder.kMeansClustering(k,normalizedWeights)\n",
        "\n",
        "    print(\"Result Of Clusters:\")\n",
        "    print(clusters)\n",
        "    print()\n",
        "\n",
        "    len0=0\n",
        "    len1=0\n",
        "    for i in range(len(clusters)):\n",
        "      if(clusters[i]==0):\n",
        "        len0+=1\n",
        "      else:\n",
        "        len1+=1\n",
        "\n",
        "    # print(len0)\n",
        "    # print(len1)\n",
        "\n",
        "    if(len1==1):\n",
        "      for i in range(len(clusters)):\n",
        "        if(clusters[i]==1):\n",
        "          print(\"Anomoly found using Clustering:\",i+1)\n",
        "          break\n",
        "    elif(len0==1):\n",
        "      for i in range(len(clusters)):\n",
        "        if(clusters[i]==0):\n",
        "          print(\"Anomoly found using Clustering:\",i+1)\n",
        "          break\n",
        "    else:\n",
        "      if(len0==5 and len1==5 or len0==4 and len1==6 or len1==4 and len0==6):\n",
        "        print(\"There are NO anomalies\")\n",
        "      else:\n",
        "        print(\"Couldn't Find Using Clustering, There may not be any anomalies.\")\n",
        "        print(\"Can't be sure need to check data to be sure \")\n",
        "\n",
        "    \n",
        "    if(len0<len1):\n",
        "      for i in range(len(clusters)):\n",
        "        if(clusters[i]==0):\n",
        "          clusters[i]=0.7\n",
        "        else:\n",
        "          clusters[i]=0.3\n",
        "\n",
        "    if(len0>len1):\n",
        "      for i in range(len(clusters)):\n",
        "        if(clusters[i]==0):\n",
        "          clusters[i]=0.3\n",
        "        else:\n",
        "          clusters[i]=0.7\n",
        "  \n",
        "    \n",
        "\n",
        "    #Euclidean Result\n",
        "    print()\n",
        "    distances=anamolyFinder.euclideanDistance(weights_array1)\n",
        "    print(\"Result Of Euclidean:\")\n",
        "    print(distances)\n",
        "\n",
        "    max=-23123213\n",
        "    answer=0\n",
        "    for i in range(len(distances)):\n",
        "      if(max<distances[i]):\n",
        "        max=distances[i]\n",
        "        answer=i+1\n",
        "\n",
        "    print(\"Anomoly found Using Euclidean:\",answer)\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "    #Manhattan Result\n",
        "    print()\n",
        "    distancesMan=anamolyFinder.manhattanDistance(weights_array1)\n",
        "    print(\"Result Of Manhattan:\")\n",
        "    print(distancesMan)\n",
        "\n",
        "    max=-23123213\n",
        "    answer=0\n",
        "    for i in range(len(distances)):\n",
        "      if(max<distancesMan[i]):\n",
        "        max=distancesMan[i]\n",
        "        answer=i+1\n",
        "\n",
        "    print(\"Anomoly found Using Manhatten:\",answer)\n",
        "\n",
        "    print()\n",
        "\n",
        "    clusters = np.array(clusters)\n",
        "    distances = np.array(distances)\n",
        "    distancesMan = np.array(distancesMan)\n",
        "    result=(clusters*0.2)+(distances*0.5)+(distancesMan*0.3)\n",
        "\n",
        "    max=-23123213\n",
        "    answer=0\n",
        "    for i in range(len(result)):\n",
        "      if(max<result[i]):\n",
        "        max=result[i]\n",
        "        answer=i+1\n",
        "\n",
        "    print(\"Anomoly Detected Using Weighted Average : \",answer)\n",
        "\n",
        "    #Cosine Similarity Result\n",
        "    print()\n",
        "    for i in range(numberOfClients-1):\n",
        "      for j in range(i+1,numberOfClients):\n",
        "        similarity=anamolyFinder.cosineSimilarity(flattendWeightedArray[i],flattendWeightedArray[j])\n",
        "        print(\"USER:\"+str(i)+\" USER:\"+str(j))\n",
        "        print(similarity)\n",
        "        print(\"!!!!!!!!!!!!!!!\")\n",
        "\n",
        "\n",
        "    print(np.shape(WeightsForServer1))\n",
        "    print(len(WeightsForServer1))\n",
        "    model=Model()\n",
        "    model=Model.modelDef(4)\n",
        "    model=Model.ModelSetWeights(model,WeightsForServer1)\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Obtain the predicted outputs of the model\n",
        "    y_pred = model.predict(test_data)\n",
        "\n",
        "    for i in range(len(y_pred)):\n",
        "      if(y_pred[i]>0.5):\n",
        "        y_pred[i]=1\n",
        "      else:\n",
        "        y_pred[i]=0\n",
        "\n",
        "    #print(y_pred)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(test_label, y_pred)\n",
        "\n",
        "    print(\"Accuracy of the model After Changing Data: {:.2f}%\".format(accuracy * 100))\n",
        "    #weightsAfterTraining,weightsOne=Model.ModelFit(model,train_data[i],train_label_data[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne=client.trainClient(numberOfClients,randomWeights,units,2,train_data_copy,train_label_data_copy,answer-1)\n",
        "    WeightsForServerAfterDropping=server.dropAvg(weightAfterTraining,biasAfterTraining,weightAfterTrainingOne,biasAfterTrainingOne,answer)\n",
        "\n",
        "    model=Model()\n",
        "    model=Model.modelDef(4)\n",
        "    model=Model.ModelSetWeights(model,WeightsForServerAfterDropping)\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Obtain the predicted outputs of the model\n",
        "    y_pred = model.predict(test_data)\n",
        "    \n",
        "\n",
        "    for i in range(len(y_pred)):\n",
        "      if(y_pred[i]>0.5):\n",
        "        y_pred[i]=1\n",
        "      else:\n",
        "        y_pred[i]=0\n",
        "\n",
        "    #print(y_pred)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    accuracy = accuracy_score(test_label, y_pred)\n",
        "\n",
        "    print(\"Accuracy of the model After dropping client: {:.2f}%\".format(accuracy * 100))\n",
        "    #weightsAfterTraining,weightsOne=Model.ModelFit(model,train_data[i],train_label_data[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OO4BImDgdGka"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}